{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch的自动求导机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad = True)\n",
    "a = torch.tensor([4.0], requires_grad = True)\n",
    "print(x.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()  # 反向传播，计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简单的线性函数的拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造数据\n",
    "x = torch.randn(100)\n",
    "y = 3 *x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数\n",
    "k = torch.tensor([0.0], requires_grad=True)\n",
    "b = torch.tensor([0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 迭代一次\n",
    "y_pre = k*x + b\n",
    "y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.015  # 设置学习速率\n",
    "# 定义损失函数为MSE\n",
    "def loss_fun(y_true, y_pre):\n",
    "    return torch.sum(torch.square(y_pre-y_true))/len(y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fun(y, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7098, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反向传播计算梯度\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.8855])\n",
      "tensor([-2.3815])\n"
     ]
    }
   ],
   "source": [
    "# 获取梯度\n",
    "print(k.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新参数\n",
    "k.data -= k.grad * lr\n",
    "b.data -= b.grad * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0733], requires_grad=True)\n",
      "tensor([0.0357], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(k)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 完成一个batch将梯度置零\n",
    "k.grad.zero_()\n",
    "b.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0, k: 0.1447, b: 0.0710, training loss: 9.2719\n",
      "Iter: 100, k: 2.7255, b: 1.7471, training loss: 0.1060\n",
      "Iter: 200, k: 2.9706, b: 1.9713, training loss: 0.0013\n",
      "Iter: 300, k: 2.9968, b: 1.9968, training loss: 0.0000\n",
      "Iter: 400, k: 2.9996, b: 1.9996, training loss: 0.0000\n",
      "Iter: 500, k: 3.0000, b: 2.0000, training loss: 0.0000\n",
      "Iter: 600, k: 3.0000, b: 2.0000, training loss: 0.0000\n",
      "Iter: 700, k: 3.0000, b: 2.0000, training loss: 0.0000\n",
      "Iter: 800, k: 3.0000, b: 2.0000, training loss: 0.0000\n",
      "Iter: 900, k: 3.0000, b: 2.0000, training loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 迭代多次\n",
    "for i in range(1000):\n",
    "    y_pre = k*x + b\n",
    "    # print(y_pre)\n",
    "    loss = loss_fun(y, y_pre)\n",
    "    loss.backward()\n",
    "    # 获取梯度\n",
    "    # print(k.grad)\n",
    "    # print(b.grad)\n",
    "    # 更新参数\n",
    "    k.data -= k.grad * lr\n",
    "    b.data -= b.grad * lr\n",
    "    # 每训练10次打印一次记录\n",
    "    if i % 100 == 0:\n",
    "        print(\"Iter: %d, k: %.4f, b: %.4f, training loss: %.4f\" %\n",
    "              (i, k.item(), b.item(), loss.item()))\n",
    "    k.grad.data.zero_()\n",
    "    b.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch套路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型构造类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinear:\n",
    "    # 初始化参数\n",
    "    def __init__(self):\n",
    "        self.k = torch.tensor([0.0], requires_grad=True)\n",
    "        self.b = torch.tensor([0.0], requires_grad=True)\n",
    "       \n",
    "    # 模型前向计算\n",
    "    def forward(self, x):\n",
    "        y = self.k * x + self.b\n",
    "        return y\n",
    "    \n",
    "    # 模型参数\n",
    "    def parameters(self):\n",
    "        return [self.k, self.b]\n",
    "    \n",
    "    # 调用模型自动开始计算\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义优化器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    # 初始化参数\n",
    "    def __init__(self, parameters, lr):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        \n",
    "    # 更新参数\n",
    "    def step(self):\n",
    "        for para in self.parameters:\n",
    "            para.data -= para.grad * self.lr\n",
    "    \n",
    "    # 初始化梯度\n",
    "    def zero_grad(self):\n",
    "        for para in self.parameters:\n",
    "            para.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数为MSE\n",
    "def loss_fun(y_true, y_pre):\n",
    "    return torch.sum(torch.square(y_pre-y_true))/len(y_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0, k:0.0733, b:0.0357, loss:9.709770\n",
      "Epoch:50, k:2.1161, b:1.2598, loss:1.015905\n",
      "Epoch:100, k:2.7192, b:1.7416, loss:0.110783\n",
      "Epoch:150, k:2.9086, b:1.9124, loss:0.012179\n",
      "Epoch:200, k:2.9699, b:1.9707, loss:0.001341\n",
      "Epoch:250, k:2.9901, b:1.9902, loss:0.000148\n",
      "Epoch:300, k:2.9967, b:1.9968, loss:0.000016\n",
      "Epoch:350, k:2.9989, b:1.9989, loss:0.000002\n",
      "Epoch:400, k:2.9996, b:1.9996, loss:0.000000\n",
      "Epoch:450, k:2.9999, b:1.9999, loss:0.000000\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\n",
    "model = SimpleLinear()\n",
    "# 实例化优化器\n",
    "opt = Optimizer(model.parameters(), lr=0.015)\n",
    "for epoch in range(500):\n",
    "    # 计算模型输出\n",
    "    output = model(x)\n",
    "    # 计算损失值\n",
    "    loss = loss_fun(y, output)\n",
    "    # 反向传播，根据计算图计算每个参数的梯度\n",
    "    loss.backward()\n",
    "    # 参数更新\n",
    "    opt.step()\n",
    "    # 参数梯度归零\n",
    "    opt.zero_grad()\n",
    "    if epoch % 50 == 0:\n",
    "        print('Epoch:{0}, k:{1:.4f}, b:{2:.4f}, loss:{3:.6f}'.format(epoch, model.parameters()[0].item(), model.parameters()[1].item(), loss))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "239.033px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
