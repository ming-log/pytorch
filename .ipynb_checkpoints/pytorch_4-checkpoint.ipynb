{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python生成进度条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义格式化输出函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------------------------------->]\n",
      "[-------------------------------------------------->]\n"
     ]
    }
   ],
   "source": [
    "# 进度条\n",
    "def jindutiao():\n",
    "    for i in range(1, 51):\n",
    "        time.sleep(0.2)\n",
    "        print('\\r[%-51s]' % ('-' * i + '>'), end='')\n",
    "    print()\n",
    "for i in range(2):\n",
    "    jindutiao()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用tqdm工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:10<00:00,  4.94it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:10<00:00,  4.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# 使用进度条工具\n",
    "def jindutiao():\n",
    "    par = tqdm.tqdm(range(1, 51))\n",
    "    for i in par:\n",
    "        time.sleep(0.2)\n",
    "for i in range(2):\n",
    "    jindutiao()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用progressbar工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from progressbar import *\n",
    " \n",
    "progress = ProgressBar()\n",
    "for i in progress(range(1000)):\n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax、logsoftmax、nllloss和CrossEntropyLoss之间的区别与联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogSoftmax\n",
    "其实就是将Softmax取对数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{LogSoftmax}(x_{i}) = \\log(\\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLLLoss\n",
    "该函数全称是negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$NLLLoss(x, class)=-x[class]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$CrossEntropyLoss=\\sum_k(p_k*\\log q_k)$$\n",
    "其中，p表示真实值，在这个公式中是ont-hot形式；q是预测值，在这里假设已经是经过softmax后的结果。\n",
    "\n",
    "仔细观察可以知道，因为p的元素不是0就是1，而且又是乘法，所以很自然地我们如果知道1所对应的index，那么就不用做其他无意义的运算了。所以在pytorch代码中target不是以**one-hot**形式表示的，而是直接用**scalar**表示。所以交叉熵的公式(m表示真实类别)可变形为：\n",
    "$$CrossEntropyLoss=\\sum_k(p_k*\\log q_k)=-log q_m $$\n",
    "仔细观察不难发现，**CrossEntropyLoss**其实就是等同于**LogSoftmax**和**NLLLoss**两个步骤。\n",
    "\n",
    "所以Pytorch中的**F.cross_entropy**会自动调用上面介绍的**LogSoftmax**和**NLLLoss**来计算交叉熵,其计算方式如下:\n",
    "$$CrossEntropyLoss(x, class)=-log(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "在分类问题中，如果模型经过全连接后直接输出结果，那么在选用loss函数时应该选用CrossEntropyLoss；如果模型经过全连接后使用log_softmax函数，那么选用loss函数是应该选用NLLLoss。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现演示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0113, -0.2398, -1.1892],\n",
       "        [-0.8600, -1.0444, -0.0218],\n",
       "        [-1.1908,  1.5461,  0.1521],\n",
       "        [-0.1471, -0.5752,  0.0168]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "input_tensor = torch.randn(4,3)\n",
    "input_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-3ab1bf8f09c2>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  sfm = F.softmax(input_tensor)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2500, 0.5408, 0.2093],\n",
       "        [0.2413, 0.2007, 0.5580],\n",
       "        [0.0493, 0.7617, 0.1890],\n",
       "        [0.3534, 0.2303, 0.4163]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm = F.softmax(input_tensor)\n",
    "sfmarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可以看出，softmax函数得到的结果为一些概率值，并且每行tensor的和为1，故其结果十分有意义，可理解为某条记录为对应类别的概率。\n",
    "\n",
    "举个例子：如果是图片分类任务，输入m张图片，输出为一个$m*N$ 的 tensor。上述中的tensor可以表示为输入4张图片，类别数为3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再特殊一点，输入的四张图片，有3个类别分别为猫、狗和猪。可以看出第一张和第三张更有可能是狗。第二张和第四张更有可能是猪。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logsoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3863, -0.6148, -1.5642],\n",
       "        [-1.4216, -1.6060, -0.5834],\n",
       "        [-3.0091, -0.2722, -1.6662],\n",
       "        [-1.0402, -1.4683, -0.8764]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(sfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nll_loss\n",
    "假设标签值为**[1,2,1,2]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5867)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5867"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用公式手动计算\n",
    "target=torch.tensor([1,2,1,2])\n",
    "sum_nll = 0\n",
    "for i in range(len(target)):\n",
    "    sum_nll += -log_sfm[i][target[i]]\n",
    "print(sum_nll/4)\n",
    "(0.6148+0.5834+0.2722+0.8764)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5867)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调用函数计算\n",
    "target=torch.tensor([1,2,1,2])\n",
    "F.nll_loss(log_sfm, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5867)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(input_tensor, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拓展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax上溢和下溢问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 什么是上溢和下溢？\n",
    "实数在计算机内用二进制表示，所以不是一个精确值，当数值过小的时候，被四舍五入为0，这个就是下溢问题。反之，当数值过大时就会出现上溢问题。\n",
    "-softmax函数中需要进行指数运算$e^c$,当c极其大，$e^c$进行指数运算后会更大，导致计算机无法存储从而造成上溢。\n",
    "-反义，当c及其小时，$e^c$就会无线趋近于0，当小数位到达一定的程度后，计算机就会将其四舍五入为0，导致下溢出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [1000.,500.,500.]\n",
    "inp1 = [-1000., -1000., -1000.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def softmax(inp):\n",
    "    length = len(inp)\n",
    "    exps = []\n",
    "    res = 0\n",
    "    ind = 0\n",
    "    for item in inp:\n",
    "        exp = math.exp(item)\n",
    "        res = res + exp\n",
    "        exps.append(exp)\n",
    "        ind+=1\n",
    "    exps = np.array(exps)   \n",
    "    return exps/res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当指数函数里面传入的值很大时，就会出现上溢，爆出OverflowError."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "math range error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-fab6275aeee7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 上溢出\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-cae78aadd3c7>\u001b[0m in \u001b[0;36msoftmax\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mexps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOverflowError\u001b[0m: math range error"
     ]
    }
   ],
   "source": [
    "softmax(inp)  # 上溢出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当指数函数里面传入的值是很小的负数时，就会出现下溢，输出结果就是0， 这样就有可能导致分母的值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-cae78aadd3c7>:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return exps/res\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(inp1)   # 下溢出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解决办法\n",
    "使用x-max(x)来代替x解决，这样就可以保证分子中最大值为0，并且分布中至少有一个1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(inp):\n",
    "    length = len(inp)\n",
    "    exps = []\n",
    "    res = 0\n",
    "    ind = 0\n",
    "    for item in inp:\n",
    "        exp = math.exp(item-max(inp))\n",
    "        res = res + exp\n",
    "        exps.append(exp)\n",
    "        ind+=1\n",
    "    exps = np.array(exps)   \n",
    "    return exps/res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+000, 7.12457641e-218, 7.12457641e-218])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(inp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见上溢出和下溢出现象都已经解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用pytorch查看是否会出现上溢和下溢问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-4fe6a6246006>:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(inp_tensor)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0.])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_tensor = torch.tensor(inp)\n",
    "inp1_tensor = torch.tensor(inp1)\n",
    "F.softmax(inp_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-2dd6e5c720df>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(inp1_tensor)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.3333])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(inp1_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见pytorch已经是经过处理了，不会出现上溢和下溢问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，经过该处理后，计算logsoftmax值时，也不会出现log(0)$->$负无穷大的情况\n",
    "$$\\text{LogSoftmax}(x_{i}) = \\log(\\frac{\\exp({x_i-M})}{\\sum_j \\exp(x_j-M)})$$\n",
    "$$=log(\\exp(x_j-M))-log(\\sum_j\\exp(x_j-M))=(x_j-M)-\\log(\\sum_j\\exp(x_j-M))$$\n",
    "此时，指数中的求和至少有一个为1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "264.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
